

-------
schema: 
-------
curl -X 'POST' \
  'http://localhost:9000/schemas?override=true&force=false' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "schemaName": "transcript",
  "dimensionFieldSpecs": [
    {
      "name": "studentID",
      "dataType": "INT"
    },
    {
      "name": "firstName",
      "dataType": "STRING"
    },
    {
      "name": "lastName",
      "dataType": "STRING"
    },
    {
      "name": "gender",
      "dataType": "STRING"
    },
    {
      "name": "subject",
      "dataType": "STRING"
    }
  ],
  "metricFieldSpecs": [
    {
      "name": "score",
      "dataType": "FLOAT"
    }
  ],
  "dateTimeFieldSpecs": [{
    "name": "timestampInEpoch",
    "dataType": "LONG",
    "format" : "1:MILLISECONDS:EPOCH",
    "granularity": "1:MILLISECONDS"
  }]
}'

------------------------

-------
table: 
-------
curl -X 'POST' \
  'http://localhost:9000/tables' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "tableName": "transcript",
  "tableType": "REALTIME",
  "segmentsConfig": {
    "timeColumnName": "timestampInEpoch",
    "timeType": "MILLISECONDS",
    "schemaName": "transcript",
    "replicasPerPartition": "1"
  },
  "tenants": {},
  "tableIndexConfig": {
    "loadMode": "MMAP",
    "streamConfigs": {
      "streamType": "kafka",
      "stream.kafka.consumer.type": "lowlevel",
      "stream.kafka.topic.name": "transcript-topic",
      "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
      "stream.kafka.broker.list": "kafka:9092",
      "realtime.segment.flush.threshold.rows": "0",
      "realtime.segment.flush.threshold.time": "24h",
      "realtime.segment.flush.threshold.segment.size": "50M",
      "stream.kafka.consumer.prop.auto.offset.reset": "smallest"
    }
  },
  "metadata": {
    "customConfigs": {}
  }
}'
------------------------


-------
kafka topic: 
-------
docker exec -it kafka sh

cd /opt/bitnami/kafka/bin

kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --list 
kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --delete --topic transcript-topic
kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --create --topic transcript-topic

kafka-console-producer.sh --topic transcript-topic --bootstrap-server 127.0.0.1:9094

{"studentID":1105,"firstName":"2Natalie","lastName":"Jones","gender":"Female","subject":"Maths","score":3.8,"timestampInEpoch":1571900400000}



val pinotDF = spark.read.format("pinot").option("table", "airlineStats").option("tableType", "OFFLINE").option("controller", "localhost:9000").load()



-------
trino create catalog : 
-------
docker exec -it trino trino


 CREATE CATALOG pinotCatalog1 USING pinot
 WITH (
    "pinot.controller-urls" = 'pinot-controller:9000'
 );

 select firstName  from pinotCatalog1.default.transcript limit 1;



 -------
 superset
 --------
 connect to trino DB : trino://trino@host.docker.internal:18080 


-----------
spark-shell
-----------

docker exec -it spark-iceberg sh

spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.4

import org.apache.spark.sql.types.{FloatType,IntegerType,StringType,StructType,StructField}

val df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kafka:9092").option("subscribe", "transcript-topic").load()

// # Define the schema for parsing the JSON
val schema = StructType(Array(StructField("firstName", StringType, true),StructField("lastName", StringType, true),StructField("subject", StringType, true),StructField("score", FloatType, true)))


val key_value = df.select(from_json(col("value").cast("string"), schema).alias("a")).select(col("a.firstName"),col("a.lastName"), col("a.score")).groupBy("firstName","lastName").agg(count("firstName").alias("count"),max("score").alias("max"))


// val key_value = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").as[(String, String)]
key_value.writeStream.outputMode("complete").format("console").start()
query.awaitTermination()


key_value.writeStream.format("iceberg").outputMode("append").option("checkpointLocation", "/home/iceberg/notebooks/notebooks").toTable("demo.shyam.events")