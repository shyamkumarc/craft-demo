
-------
schema: 
-------
curl -X 'POST' \
  'http://localhost:9000/schemas?override=true&force=false' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "schemaName": "logs",
  "dimensionFieldSpecs": [
    {
      "name": "thread",
      "dataType": "STRING"
    },
    {
      "name": "loggingMode",
      "dataType": "STRING"
    },
    {
      "name": "class",
      "dataType": "STRING"
    },
    {
      "name": "message",
      "dataType": "STRING"
    }
  ],
  "metricFieldSpecs": [
    {
      "name": "memoryUsage",
      "dataType": "FLOAT"
    }
  ],
  "dateTimeFieldSpecs": [{
    "name": "timestampInEpoch",
    "dataType": "LONG",
    "format" : "1:MILLISECONDS:EPOCH",
    "granularity": "1:MILLISECONDS"
  }]
}'

------------------------

-------
table: 
-------
curl -X 'POST' \
  'http://localhost:9000/tables' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "tableName": "logs",
  "tableType": "REALTIME",
  "segmentsConfig": {
    "timeColumnName": "timestampInEpoch",
    "timeType": "MILLISECONDS",
    "schemaName": "logs",
    "replicasPerPartition": "1"
  },
  "tenants": {},
  "fieldConfigList":[
    {
      "name":"message",
      "encodingType":"RAW",
      "indexTypes":["TEXT"]
    }
    ],
  "tableIndexConfig": {
    "noDictionaryColumns": [
     "message"
     ],
    "loadMode": "MMAP",
    "streamConfigs": {
      "streamType": "kafka",
      "stream.kafka.consumer.type": "lowlevel",
      "stream.kafka.topic.name": "logs-topic",
      "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
      "stream.kafka.broker.list": "kafka:9092",
      "realtime.segment.flush.threshold.rows": "0",
      "realtime.segment.flush.threshold.time": "24h",
      "realtime.segment.flush.threshold.segment.size": "50M",
      "stream.kafka.consumer.prop.auto.offset.reset": "smallest"
    }
  },
  "metadata": {
    "customConfigs": {}
  }
}'
------------------------


-------
kafka topic: 
-------
docker exec -it kafka sh

cd /opt/bitnami/kafka/bin

kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --list 
kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --delete --topic logs-topic
kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --create --topic logs-topic

kafka-console-producer.sh --topic logs-topic --bootstrap-server 127.0.0.1:9094

{"thread":"thread-1","loggingMode":"SUCCESS","class":"com.shyam.demo1","message":"this is some random log text","memoryUsage":11, "timestampInEpoch":1571900400000}


-------
trino create catalog : 
-------
docker exec -it trino trino


 select *  from pinot.default.logs limit 1;
  select *  from iceberg.report.logs limit 1;

------------------------
Pinot text search query:
------------------------
SELECT message 
FROM logs 
WHERE TEXT_MATCH(message, 'user OR received')




 -------
 superset
 --------
 connect to trino DB : trino://trino@host.docker.internal:18080 


-----------
spark-shell
-----------

docker exec -it spark-iceberg sh

spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.4

import org.apache.spark.sql.types.{IntegerType,StringType,StructType,StructField}

val df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kafka:9092").option("subscribe", "logs-topic").load()

val schema = StructType(Array(StructField("class", StringType, true),StructField("loggingMode", StringType, true),StructField("memoryUsage", StringType , true),StructField("message", StringType , true)))

val errorLogs = df.select(from_json(col("value").cast("string"), schema).alias("a")).select(col("a.class"),col("a.loggingMode"),col("a.message"), col("a.memoryUsage").cast("float")).filter("a.memoryUsage > 200")

//key_value.writeStream.outputMode("append").format("console").start()
//query.awaitTermination()


errorLogs.writeStream.format("iceberg").outputMode("append").option("checkpointLocation", "/home/iceberg/notebooks/notebooks").toTable("demo.report.logs")